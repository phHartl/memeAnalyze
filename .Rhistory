# left_join(get_sentiments("nrc"), by = "word") %>%
group_by(templateName, templateMakros) %>%
summarize(totalTokens= n()) %>%
mutate(averageTokens=round((totalTokens/templateMakros), digits=0))%>%
ungroup() %>%
write.csv(.,file = "R/csv-out/total_and_average_tokens_in_templates.csv")
# Word/Token counts
ave_words_count <- memes %>%
group_by(templateName) %>%
mutate(templateMakros= n()) %>%
ungroup() %>%
unnest_tokens(token, text) %>%  # Tokenization
#anti_join(stopwords_watermarks, by = "token") %>%     # Excluding stop words
# filter(!grepl('[0-9]', word)) %>%         # Excluding numbers
# left_join(get_sentiments("nrc"), by = "word") %>%
group_by(templateName, templateMakros) %>%
summarize(totalTokens= n()) %>%
mutate(averageTokens=round((totalTokens/templateMakros), digits=0))%>%
ungroup() %>%
write.csv(.,file = "R/csv-out/total_and_average_tokens_in_templates.csv")
# Word/Token counts
ave_words_count <- memes %>%
group_by(templateName) %>%
mutate(templateMakros= n()) %>%
ungroup() %>%
unnest_tokens(token, text) %>%  # Tokenization
#anti_join(stopwords_watermarks, by = "token") %>%     # Excluding stop words
# filter(!grepl('[0-9]', word)) %>%         # Excluding numbers
# left_join(get_sentiments("nrc"), by = "word") %>%
group_by(templateName, templateMakros) %>%
summarize(totalTokens= n()) %>%
mutate(averageTokens=round((totalTokens/templateMakros), digits=0))%>%
ungroup() #%>%
View(ave_words_count)
# Word/Token counts
ave_words_count <- memes %>%
group_by(templateName) %>%
mutate(templateMakros= n()) %>%
ungroup() %>%
unnest_tokens(token, text) %>%  # Tokenization
anti_join(stopwords_watermarks, by = "text") %>%     # Excluding stop words
# filter(!grepl('[0-9]', word)) %>%         # Excluding numbers
# left_join(get_sentiments("nrc"), by = "word") %>%
group_by(templateName, templateMakros) %>%
summarize(totalTokens= n()) %>%
mutate(averageTokens=round((totalTokens/templateMakros), digits=0))%>%
ungroup() #%>%
# Word/Token counts
ave_words_count <- memes %>%
group_by(templateName) %>%
mutate(templateMakros= n()) %>%
ungroup() %>%
unnest_tokens(token, text) #%>%  # Tokenization
View(ave_words_count)
# Word/Token counts
ave_words_count <- memes %>%
group_by(templateName) %>%
mutate(templateMakros= n()) %>%
ungroup() %>%
unnest_tokens(token, text) %>%  # Tokenization
anti_join(stopwords_watermarks, by = "token") %>%     # Excluding stop words
# filter(!grepl('[0-9]', word)) %>%         # Excluding numbers
# left_join(get_sentiments("nrc"), by = "word") %>%
group_by(templateName, templateMakros) %>%
summarize(totalTokens= n()) %>%
mutate(averageTokens=round((totalTokens/templateMakros), digits=0))%>%
ungroup() #%>%
# Word/Token counts
ave_words_count <- memes %>%
group_by(templateName) %>%
mutate(templateMakros= n()) %>%
ungroup() %>%
unnest_tokens(token, text) %>%  # Tokenization
anti_join(stopwords_watermarks,by=c("token"="X1")) %>%     # Excluding stop words
# filter(!grepl('[0-9]', word)) %>%         # Excluding numbers
# left_join(get_sentiments("nrc"), by = "word") %>%
group_by(templateName, templateMakros) %>%
summarize(totalTokens= n()) %>%
mutate(averageTokens=round((totalTokens/templateMakros), digits=0))%>%
ungroup() #%>%
View(ave_words_count)
# Word/Token counts
ave_words_count <- memes %>%
group_by(templateName) %>%
mutate(templateMakros= n()) %>%
ungroup() %>%
unnest_tokens(token, text) %>%  # Tokenization
#anti_join(stopwords_watermarks,by=c("token"="X1")) %>%     # Excluding stop words
# filter(!grepl('[0-9]', word)) %>%         # Excluding numbers
# left_join(get_sentiments("nrc"), by = "word") %>%
group_by(templateName, templateMakros) %>%
summarize(totalTokens= n()) %>%
mutate(averageTokens=round((totalTokens/templateMakros), digits=0))%>%
ungroup() #%>%
# Word/Token counts
ave_words_count <- memes %>%
group_by(templateName) %>%
mutate(templateMakros= n()) %>%
ungroup() %>%
unnest_tokens(token, text) %>%  # Tokenization
anti_join(stopwords_watermarks,by=c("token"="X1")) %>%     # Excluding stop words
# filter(!grepl('[0-9]', word)) %>%         # Excluding numbers
# left_join(get_sentiments("nrc"), by = "word") %>%
group_by(templateName, templateMakros) %>%
summarize(totalTokens= n()) %>%
mutate(averageTokens=round((totalTokens/templateMakros), digits=0))%>%
ungroup() %>%
write.csv(.,file = "R/csv-out/total_and_average_tokens_in_templates.csv")
library(reshape2)
library(wordcloud)
library(readr)
library(gplots)
require(tidyverse)
require(tidytext)
require(RColorBrewer)
require(ggplot2)
# https://www.tidytextmining.com/sentiment.html
# Import data
memes <- read_csv("nodeyourmeme/memes.csv")
#View(memes)
memes <- memes%>%filter(!is.na(text))
memes_without_text <- memes%>%filter(is.na(text))
# Loading clean function for using in mutate
source("R/clean_text.R")
# Reading stopwords list
stopwords <- read_csv("R/stopword_lists/stopwords.txt", col_names = FALSE)
stopwords_custom <- read_csv("R/stopword_lists/stopwords_custom.txt", col_names = FALSE)
stopwords_watermarks <- read_csv("R/stopword_lists/stopwords_watermarks.txt", col_names = FALSE)
##############################################
meme_template_texts<-memes%>%
# meme template topic model / Taking all memes with same meme-template origin
group_by(templateName, meme_id=rownames(memes))%>%
summarise(newDoc=paste0(text,collapse=" "))%>%
ungroup()%>%
mutate(newDoc=clean_text(newDoc))%>%
filter(str_count(newDoc)>3)%>%
group_by(templateName)%>%
mutate(index=row_number())%>%
ungroup()
meme_template_words<-meme_template_texts%>%
unnest_tokens(word, newDoc)
############
# Occurances
# Word/Token counts
ave_words_count <- memes %>%
group_by(templateName) %>%
mutate(templateMakros= n()) %>%
ungroup() %>%
unnest_tokens(token, text) %>%  # Tokenization
anti_join(stopwords_watermarks,by=c("token"="X1")) %>%     # Excluding stop words
# filter(!grepl('[0-9]', word)) %>%         # Excluding numbers
# left_join(get_sentiments("nrc"), by = "word") %>%
group_by(templateName, templateMakros) %>%
summarize(totalTokens= n()) %>%
mutate(averageTokens=round((totalTokens/templateMakros), digits=0))%>%
ungroup() %>%
write.csv(.,file = "R/csv-out/total_and_average_tokens_in_templates.csv")
most_frequent_words <- memes %>%
unnest_tokens(token, text) %>%  # Tokenization
group_by(templateName, token) %>%
summarize(occurances= n()) %>%
ungroup()%>%
group_by(templateName) %>%
top_n(10) %>%
write.csv(.,file = "R/csv-out/10_most_frequent_tokens_in_templates.csv")
####
word_freq_per_template <- meme_template_words %>%
anti_join(stopwords_custom,by=c("word"="X1")) %>%
group_by(word, templateName) %>%
summarise(n=n()) %>%
ungroup() %>%
group_by(templateName) %>%
top_n(10)
word_freq_total <- meme_template_words %>%
anti_join(stopwords,by=c("word"="X1")) %>%
group_by(word) %>%
summarise(n=n())
############
# Wordclouds
pal<-brewer.pal(7, "Set3")
pal2<-brewer.pal(8, "Dark2")
meme_template_words %>%
anti_join(stopwords_custom,by=c("word"="X1")) %>%
count(word)
meme_template_words %>%
anti_join(stopwords_custom,by=c("word"="X1")) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 250))
meme_template_words %>%
anti_join(stopwords_custom,by=c("word"="X1")) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 100))
png("wordcloud.png", width = 12, height = 8, units = 'in', res = 300)
res <-wordcloud(word_freq_total$word,
word_freq_total$n,
max.words = Inf,
scale = c(8,.02),
random.order = FALSE,
rot.per = 0.15,
colors=pal2)
library(readr)
X10_most_frequent_tokens_in_templates <- read_csv("R/csv-out/10_most_frequent_tokens_in_templates.csv")
View(X10_most_frequent_tokens_in_templates)
library(reshape2)
library(wordcloud)
library(readr)
library(gplots)
require(tidyverse)
require(tidytext)
require(RColorBrewer)
require(ggplot2)
# https://www.tidytextmining.com/sentiment.html
# Import data
memes <- read_csv("nodeyourmeme/memes.csv")
#View(memes)
memes <- memes%>%filter(!is.na(text))
memes_without_text <- memes%>%filter(is.na(text))
# Loading clean function for using in mutate
source("R/clean_text.R")
# Reading stopwords list
stopwords <- read_csv("R/stopword_lists/stopwords.txt", col_names = FALSE)
stopwords_custom <- read_csv("R/stopword_lists/stopwords_custom.txt", col_names = FALSE)
stopwords_watermarks <- read_csv("R/stopword_lists/stopwords_watermarks.txt", col_names = FALSE)
##############################################
meme_template_texts<-memes%>%
# meme template topic model / Taking all memes with same meme-template origin
group_by(templateName, meme_id=rownames(memes))%>%
summarise(newDoc=paste0(text,collapse=" "))%>%
ungroup()%>%
mutate(newDoc=clean_text(newDoc))%>%
filter(str_count(newDoc)>3)%>%
group_by(templateName)%>%
mutate(index=row_number())%>%
ungroup()
meme_template_words<-meme_template_texts%>%
unnest_tokens(word, newDoc)
############
# Occurances
# Word/Token counts
ave_words_count <- memes %>%
group_by(templateName) %>%
mutate(templateMakros= n()) %>%
ungroup() %>%
unnest_tokens(token, text) %>%  # Tokenization
anti_join(stopwords_watermarks,by=c("token"="X1")) %>%     # Excluding stop words
# filter(!grepl('[0-9]', word)) %>%         # Excluding numbers
# left_join(get_sentiments("nrc"), by = "word") %>%
group_by(templateName, templateMakros) %>%
summarize(totalTokens= n()) %>%
mutate(averageTokens=round((totalTokens/templateMakros), digits=0))%>%
ungroup() %>%
write.csv(.,file = "R/csv-out/total_and_average_tokens_in_templates.csv")
most_frequent_words <- memes %>%
unnest_tokens(token, text) %>%  # Tokenization
anti_join(stopwords_watermarks,by=c("token"="X1")) %>%     # Excluding stop words
group_by(templateName, token) %>%
summarize(occurances= n()) %>%
ungroup()%>%
group_by(templateName) %>%
top_n(10) %>%
write.csv(.,file = "R/csv-out/10_most_frequent_tokens_in_templates.csv")
####
word_freq_per_template <- meme_template_words %>%
anti_join(stopwords_custom,by=c("word"="X1")) %>%
group_by(word, templateName) %>%
summarise(n=n()) %>%
ungroup() %>%
group_by(templateName) %>%
top_n(10)
word_freq_total <- meme_template_words %>%
anti_join(stopwords,by=c("word"="X1")) %>%
group_by(word) %>%
summarise(n=n())
############
# Wordclouds
pal<-brewer.pal(7, "Set3")
pal2<-brewer.pal(8, "Dark2")
meme_template_words %>%
anti_join(stopwords_custom,by=c("word"="X1")) %>%
count(word)
meme_template_words %>%
anti_join(stopwords_custom,by=c("word"="X1")) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 250))
meme_template_words %>%
anti_join(stopwords_custom,by=c("word"="X1")) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 100))
png("wordcloud.png", width = 12, height = 8, units = 'in', res = 300)
res <-wordcloud(word_freq_total$word,
word_freq_total$n,
max.words = Inf,
scale = c(8,.02),
random.order = FALSE,
rot.per = 0.15,
colors=pal2)
# Word/Token counts
ave_words_count <- memes %>%
group_by(templateName) %>%
mutate(templateMakros= n()) %>%
ungroup() %>%
unnest_tokens(token, text) %>%  # Tokenization
anti_join(stopwords_watermarks,by=c("token"="X1")) %>%     # Excluding stop words
# filter(!grepl('[0-9]', word)) %>%         # Excluding numbers
# left_join(get_sentiments("nrc"), by = "word") %>%
group_by(templateName, templateMakros) %>%
summarize(totalTokens= n()) %>%
mutate(averageTokens=round((totalTokens/templateMakros), digits=0))%>%
ungroup() %>%
with(write.csv(.,file = "R/csv-out/total_and_average_tokens_in_templates.csv"))
library(reshape2)
library(wordcloud)
library(readr)
library(gplots)
require(tidyverse)
require(tidytext)
require(RColorBrewer)
require(ggplot2)
# https://www.tidytextmining.com/sentiment.html
# Import data
memes <- read_csv("nodeyourmeme/memes.csv")
#View(memes)
memes <- memes%>%filter(!is.na(text))
memes_without_text <- memes%>%filter(is.na(text))
# Loading clean function for using in mutate
source("R/clean_text.R")
# Reading stopwords list
stopwords <- read_csv("R/stopword_lists/stopwords.txt", col_names = FALSE)
stopwords_custom <- read_csv("R/stopword_lists/stopwords_custom.txt", col_names = FALSE)
stopwords_watermarks <- read_csv("R/stopword_lists/stopwords_watermarks.txt", col_names = FALSE)
##############################################
meme_template_texts<-memes%>%
# meme template topic model / Taking all memes with same meme-template origin
group_by(templateName, meme_id=rownames(memes))%>%
summarise(newDoc=paste0(text,collapse=" "))%>%
ungroup()%>%
mutate(newDoc=clean_text(newDoc))%>%
filter(str_count(newDoc)>3)%>%
group_by(templateName)%>%
mutate(index=row_number())%>%
ungroup()
meme_template_words<-meme_template_texts%>%
unnest_tokens(word, newDoc)
############
# Occurances
# Word/Token counts
ave_words_count <- memes %>%
group_by(templateName) %>%
mutate(templateMakros= n()) %>%
ungroup() %>%
unnest_tokens(token, text) %>%  # Tokenization
anti_join(stopwords_watermarks,by=c("token"="X1")) %>%     # Excluding stop words
# filter(!grepl('[0-9]', word)) %>%         # Excluding numbers
# left_join(get_sentiments("nrc"), by = "word") %>%
group_by(templateName, templateMakros) %>%
summarize(totalTokens= n()) %>%
mutate(averageTokens=round((totalTokens/templateMakros), digits=0))%>%
ungroup() %>%
write.csv(.,file = "R/csv-out/total_and_average_tokens_in_templates.csv")
most_frequent_words <- memes %>%
unnest_tokens(token, text) %>%  # Tokenization
anti_join(stopwords_watermarks,by=c("token"="X1")) %>%     # Excluding stop words
group_by(templateName, token) %>%
summarize(occurances= n()) %>%
ungroup()%>%
group_by(templateName) %>%
top_n(10) %>%
write.csv(.,file = "R/csv-out/10_most_frequent_tokens_in_templates.csv")
total_and_average_tokens_in_templates <- read_csv("R/csv-out/total_and_average_tokens_in_templates.csv")
most_frequent_words <- read_csv("R/csv-out/10_most_frequent_tokens_in_templates.csv")
gc()
####
word_freq_per_template <- meme_template_words %>%
anti_join(stopwords_custom,by=c("word"="X1")) %>%
group_by(word, templateName) %>%
summarise(n=n()) %>%
ungroup() %>%
group_by(templateName) %>%
top_n(10)
word_freq_total <- meme_template_words %>%
anti_join(stopwords,by=c("word"="X1")) %>%
group_by(word) %>%
summarise(n=n())
############
# Wordclouds
pal<-brewer.pal(7, "Set3")
pal2<-brewer.pal(8, "Dark2")
meme_template_words %>%
anti_join(stopwords_custom,by=c("word"="X1")) %>%
count(word)
meme_template_words %>%
anti_join(stopwords_custom,by=c("word"="X1")) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 250))
meme_template_words %>%
anti_join(stopwords_custom,by=c("word"="X1")) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 100))
png("wordcloud.png", width = 12, height = 8, units = 'in', res = 300)
res <-wordcloud(word_freq_total$word,
word_freq_total$n,
max.words = Inf,
scale = c(8,.02),
random.order = FALSE,
rot.per = 0.15,
colors=pal2)
meme_template_words %>%
anti_join(stopwords_custom,by=c("word"="X1")) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 100))
meme_template_words %>%
anti_join(stopwords_custom,by=c("word"="X1")) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 100))
library(wordcloud)
library(readr)
library(gplots)
require(tidyverse)
require(tidytext)
require(RColorBrewer)
require(ggplot2)
library(reshape2)
meme_template_words %>%
anti_join(stopwords_custom,by=c("word"="X1")) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 100))
meme_template_words %>%
anti_join(stopwords_custom,by=c("word"="X1")) %>%
count(word) %>%
with(wordcloud(word_freq_total$word,
word_freq_total$n,
max.words = 300,
scale = c(8,.02),
random.order = FALSE,
rot.per = 0.15,
colors=pal2))
meme_template_words %>%
anti_join(stopwords_custom,by=c("word"="X1")) %>%
count(word) %>%
with(wordcloud(word_freq_total$word,
word_freq_total$n,
max.words = 500,
scale = c(8,.02),
random.order = FALSE,
rot.per = 0.15,
colors=pal2))
meme_template_words %>%
anti_join(stopwords_custom,by=c("word"="X1")) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 100, random.order = FALSE))
res <-wordcloud(word_freq_total$word,
word_freq_total$n,
max.words = Inf,
scale = c(3.5,0.25),
random.order = FALSE,
rot.per = 0.15,
colors=pal2)
meme_template_words %>%
anti_join(stopwords_custom,by=c("word"="X1")) %>%
count(word) %>%
with(wordcloud(word_freq_total$word,
word_freq_total$n,
max.words = 500,
scale = c(3.5,0.25),
random.order = FALSE,
rot.per = 0.15,
colors=pal2))
meme_template_words %>%
anti_join(stopwords_custom,by=c("word"="X1")) %>%
count(word) %>%
with(wordcloud(word_freq_total$word,
word_freq_total$n,
max.words = 500,
scale = c(3.5,0.25),
random.order = FALSE,
rot.per = 0.15,
colors=pal2))
stopwords <- read_csv("R/stopword_lists/stopwords.txt", col_names = FALSE)
stopwords_custom <- read_csv("R/stopword_lists/stopwords_custom.txt", col_names = FALSE)
stopwords_watermarks <- read_csv("R/stopword_lists/stopwords_watermarks.txt", col_names = FALSE)
meme_template_words %>%
anti_join(stopwords_custom,by=c("word"="X1")) %>%
count(word) %>%
with(wordcloud(word_freq_total$word,
word_freq_total$n,
max.words = 500,
scale = c(3.5,0.25),
random.order = FALSE,
rot.per = 0.15,
colors=pal2))
meme_template_words %>%
anti_join(stopwords_custom,by=c("word"="X1")) %>%
count(word) %>%
with(wordcloud(word_freq_total$word,
word_freq_total$n,
max.words = 500,
scale = c(3.5,0.25),
random.order = FALSE,
rot.per = 0.15,
colors=pal2))
View(stopwords_custom)
meme_template_words %>%
anti_join(stopwords_custom,by=c("word"="X1")) %>%
count(word) #%>%
View(meme_template_words)
meme_template_words %>%
anti_join(stopwords_custom,by=c("word"="X1")) #%>%
meme_template_words %>%
anti_join(stopwords_watermarks,by=c("word"="X1")) #%>%
r<-meme_template_words %>%
anti_join(stopwords_watermarks,by=c("word"="X1")) #%>%
View(r)
